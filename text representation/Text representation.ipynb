{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text representation\n",
    "\n",
    "\n",
    "____________________\n",
    "- **Name:** Maite Giménez.\n",
    "- **Mail:** mgimenez@dsic.upv.es\n",
    "- **Github:** maigimenez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Table of contents\n",
    "\n",
    "1. Definition of the Problem: Text Representation in NLP.\n",
    "2. Basic Word Representation.\n",
    "    1. Bag of words (BoW).\n",
    "    2. Bag of n-grams (n-grams).\n",
    "3. Not so Basic Word Representation.\n",
    "    1. Weigthed models (Tf-Idf).\n",
    "    2. Probabilistic language models.\n",
    "4. New approaches.\n",
    "    1. Classical approaches, classical problems. \n",
    "    2. The deep (and not so deep) learning approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Definition of the Problem: Text Representation in NLP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Machine Learning models learn from an input of numbers(images, measures) how to predict the class of input. \n",
    "\n",
    "![problem](imgs/problem.png \"Problems with NLP\")\n",
    "\n",
    "But words are NOT numbers!\n",
    "\n",
    "![problem2](imgs/problem2.png \"Problems with NLP\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practical example:  Brown Corpus\n",
    "\n",
    "- The first million-word electronic corpus of English\n",
    "- Contains text from 500 sources.\n",
    "- Sources have been categorized by genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Categories: adventure, belles_lettres, editorial, fiction, government, hobbies, humor, learned, lore, mystery, news, religion, reviews, romance, science_fiction \n",
      "\n",
      "* Sentences from a category:\n",
      "  -  The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "  -  The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "print(\"* Categories: {} \\n\".format(', '.join(brown.categories())))\n",
    "\n",
    "print(\"* Sentences from a category:\")\n",
    "for sentence in brown.sents(categories=['news'])[:2]:\n",
    "    # Sentences are a list of words\n",
    "    print(\"  - \", ' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Basic Text Representation.\n",
    "## A. Bag of words (BoW).\n",
    "\n",
    "- Simplest way to represent text.\n",
    "- A text is represented as a set of its words. \n",
    "- Does **not** consider the order of the words.\n",
    "\n",
    "- Create a vector with the size of the vocabulary seen in **train**.\n",
    "- Each sentence is represented counting the number of times each word appears(frequency). Or simply counting if the word appears or not (One-hot representation). \n",
    "- Frequencies can be normalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dataset = {\"El gato comerá pato dentro de un rato\", \n",
    "           \"El pato se esconde de un gato en un zapato\"}\n",
    "# Some preprocessing might happen \n",
    "vocabulary = {\"el\",\"gato\",\"comera\",\"pato\",\"dentro\",\"de\",\"un\",\"rato\",\"se\",\"esconde\",\"zapato\"}\n",
    "\n",
    "onehot_representation = [[1,1,1,1,1,1,1,1,0,0,0],\n",
    "                         [1,1,0,0,1,1,1,0,1,1,1]]\n",
    "\n",
    "bow_representation = [[1,1,1,1,1,1,1,1,0,0,0],\n",
    "                      [1,1,0,0,1,2,2,0,1,1,1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: {0|1}^(4623x11880)\n",
      "\n",
      "VOCABULARY EXTRACT: acid, acknowledge, acknowledged, acknowledgment, acquaint, acquaintance, acquire, acquired, acquisition, acquittal, acre, acreage, acres, acrobatic, across, act, acted, acting, action, actions, active, activities, activity, actor, actors, actress, acts, actual, actually, acute, ad, adair, adam, adamant, adams, adamson, adaptation, adapting, adc, adcock, add, added, addicts, adding, addition, additional, address, addressed, addresses, addressing\n",
      "\n",
      "TWEET REPRESENTATION: [0 0 0 ..., 0 0 0]. Size of the vector: (11880,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             ngram_range=(1, 1)) \n",
    "\n",
    "sentences = [' '.join(sentence) for sentence in brown.sents(categories=['news'])]\n",
    "sentences_token = [sentence for sentence in brown.sents(categories=['news'])]\n",
    "\n",
    "# Fit the text\n",
    "BOW_text = vectorizer.fit_transform(sentences)\n",
    "BOW_text = BOW_text.toarray()\n",
    "\n",
    "print('Text: {{0|1}}^({}x{})'.format(BOW_text.shape[0], BOW_text.shape[1]))\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print('\\nVOCABULARY EXTRACT: {}'.format(', '.join(vocab[500:550])))\n",
    "# np.set_printoptions(threshold=np.nan)\n",
    "print('\\nTWEET REPRESENTATION: {}. Size of the vector: {}'.format(BOW_text[0], BOW_text[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Basic Text Representation.\n",
    "## B. Bag of n-grams (n-grams).\n",
    "\n",
    "- A BoW approach can be atomized to use smaller elements.\n",
    "- An n-gram is a continuous sequence of *n* elements: words or characters.\n",
    "- Most common n-grams:\n",
    "    - 1-gram (unigrams)\n",
    "    - 2-grams (bigrams)\n",
    "    - 3-grams (tri|grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# WORD-BIGRAM representation\n",
    "dataset = {\"El gato comerá pato dentro de un rato\", \n",
    "          \"El pato se esconde de un gato en un zapato\"}\n",
    "# Some preprocessing might happen \n",
    "vocabulary = {\"el gato\",\"gato comera\", \"comera pato\", \"pato dentro\", \n",
    "              \"dentro de\",\"de un\",\"un rato\",\"rato <EOF>\", \"el pato\",\n",
    "              \"pato se\", \"se esconde\", \"esconde de\", \"un gato\", \"gato en\", \"en un\", \"un zapato\"}\n",
    "\n",
    "ngrams_representation = [[1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0],\n",
    "                         [0,0,0,0,0,1,0,0,1,1,1,1,1,1,1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: {0|1}^(4623x67007)\n",
      "\n",
      "VOCABULARY EXTRACT: 1913, 1913 few, 1914, 1914 the, 1917, 1917 and, 1919, 1919 white, 192, 192 865, 1920, 1920 as, 1920 presidential, 1920s, 1920s following, 1921, 1921 from, 1922, 1922 the, 1923, 1923 to, 1924, 1924 and, 1925, 1925 and, 1925 as, 1926, 1926 ne, 1927, 1927 by, 1927 fewer, 1927 ruth, 1927 season, 1927 two, 1928, 1928 and, 1930, 1930 made, 1930 they, 1930 when, 1930s, 1930s he, 1932, 1932 or, 1933, 1933 individuals, 1934, 1934 farmers, 1934 implicit, 1935\n",
      "\n",
      "TWEET REPRESENTATION: [0 0 0 ..., 0 0 0]. Size of the vector: (67007,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             ngram_range=(1, 2)) \n",
    "\n",
    "sentences = [' '.join(sentence) for sentence in brown.sents(categories=['news'])]\n",
    "sentences_token = [sentence for sentence in brown.sents(categories=['news'])]\n",
    "\n",
    "# Fit the text\n",
    "ngram_text = vectorizer.fit_transform(sentences)\n",
    "ngram_text = ngram_text.toarray()\n",
    "\n",
    "print('Text: {{0|1}}^({}x{})'.format(ngram_text.shape[0], ngram_text.shape[1]))\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print('\\nVOCABULARY EXTRACT: {}'.format(', '.join(vocab[500:550])))\n",
    "# np.set_printoptions(threshold=np.nan)\n",
    "print('\\nTWEET REPRESENTATION: {}. Size of the vector: {}'.format(ngram_text[0], ngram_text[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Not so Basic Text Representation.\n",
    "## A. Weigthed models: term frequency–inverse document frequency (Tf-Idf).\n",
    "\n",
    "- Common technique from Information Retrieval (IR)\n",
    "- This word representation tries to model how important a word is to a document in a corpus.\n",
    "- Two terms are combined (or weighted): **term frequency** and **inverse document frequency**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Term frequency\n",
    "- Count the number of times that term *t* occurs in a corpus *c*.\n",
    "$$tf(t, c) = \\frac{f(t,c)}{\\max \\{f(w,c) \\forall w \\in c \\}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Inverse document frequency\n",
    "- For avoiding that common terms with low relevance this measure is weigthed using the idf formula:\n",
    "$$idf(t, C) = \\frac{|C|}{|c \\in C : t \\in c|}$$\n",
    "$|c \\in C : t \\in c|$: number of documents where the term *t* appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Term frequency–Inverse document frequency \n",
    "$$tf-idf(t, c, C) = tf(t,c) \\times idf(t,C) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: {0|1}^(4623x5807)\n",
      "\n",
      "VOCABULARY EXTRACT: athletic, athletics, atlanta, atlantic, atmosphere, atomic, attachment, attack, attacked, attacks, attempt, attempted, attempting, attempts, attend, attended, attending, attends, attention, attitude, attorney, attorneys, attract, attracted, attraction, attractive, atty, audience, audio, auditorium, audubon, aug, august, augusta, aunt, aurora, austere, austin, author, authorities, authority, authorize, authorized, authorizing, auto, automatic, automatically, automobile, autumn, av\n",
      "\n",
      "TWEET REPRESENTATION: [0 0 0 ..., 0 0 0]. Size of the vector: (67007,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, \n",
    "                             min_df=2, \n",
    "                             stop_words='english')\n",
    "\n",
    "# Fit the text\n",
    "tfidf_text = vectorizer.fit_transform(sentences)\n",
    "tfidf_text = tfidf_text.toarray()\n",
    "\n",
    "print('Text: {{0|1}}^({}x{})'.format(tfidf_text.shape[0], tfidf_text.shape[1]))\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print('\\nVOCABULARY EXTRACT: {}'.format(', '.join(vocab[500:550])))\n",
    "# np.set_printoptions(threshold=np.nan)\n",
    "print('\\nTWEET REPRESENTATION: {}. Size of the vector: {}'.format(ngram_text[0], ngram_text[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Not so Basic Text Representation.\n",
    "## B. statistical language models.\n",
    "\n",
    "\n",
    "- A probability distribution over sequences of words.\n",
    "- Able to handle some word ordering. \n",
    "- Several assumptions are taken into account:\n",
    "    - The probability of a word *w* depends on certain history (previous words) seen.\n",
    "    - *The Markov assumption*: the probability of a word only depend on a fixed number of the previous words (1 for unigrams, 2 for bigrams, ...).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Given a sequence of length *n*, the probability that W random variables took the values of the sequence $w_1^n$ can be described using the chain rule:\n",
    "\n",
    "$$ P(w_1^n) = P(w_1)*P(w_2|w_1)*P(w_3|w_1^2)\\ldots P(w_n|w_1^{n-1}) $$\n",
    "$$ = \\prod_{i=1}^{n}{P(w_i|w_1^{i-1}) }$$\n",
    "\n",
    "- Applying the Markov assumption:\n",
    "\n",
    "$$ P(w_1^n) \\approx \\prod_{k=1}^{n}{P(w_k|w_{k-n+1}^{k-1}) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Statistical language models can deal with out-of-vocabulary words applying different smoothing techniques such as:\n",
    "    - Backoff.\n",
    "    - Linear interpolation.\n",
    "    - Good-Turing. \n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. New approaches.\n",
    "## A. Classical approaches, classical problems.\n",
    "\n",
    "- Unseen words.\n",
    "- No semantical relationships\n",
    "    - One-hot Representation: Represents every word as an $\\mathbb{R}^{|V|}$ with no semantical relationships.\n",
    "   $w^{hotel}\\, and\\, w^{motel} and\\, w^{hostel} = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hotel = [0, 0, 0, 0, 0, 0, 1, 0]\n",
    "motel = [0, 0, 0, 0, 1, 0, 0, 0]\n",
    "hostel = [1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![problem3](imgs/problem3.png \"Similar words not similar representations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The curse of dimensionality\n",
    "- Generalizing locally (eg. nearest neighbors) requires representative examples for all relevant variations\n",
    "- The number of possible configurations of the variables of interest is much larger than the number of training samples.\n",
    "\n",
    "\n",
    "![curse](imgs/curse.jpg \"The curse of dimensionality\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. New approaches.\n",
    "## B. The deep (and not so deep) learning approach.\n",
    "\n",
    "> You shall know a word by the company it keeps.\n",
    ">\n",
    "> -- <cite>John Rupert Firth</cite>\n",
    "![curse](imgs/firth.png \"John Rupert Firth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word2vec\n",
    "\n",
    "   - Word2vec is a family of algorithms for learning word embeddings. You may want to talk about a skipgram model trained using negative sampling.\n",
    "    - Word2vec is NOT the holy grail (If you don't understand it)\n",
    "    \n",
    "    > Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- With Word2vec algorithms are able to learn word embedings from unsupervised raw text.\n",
    "- Two algorithms are implemented:\n",
    "    - Continuous Bag-of-Words model (CBOW)\n",
    "        - Predicts target words (e.g. 'mat') from source context words ('the cat sits on the').\n",
    "        - CBOW smoothes over a lot of the distributional information.\n",
    "        - Useful for smaller datasets.\n",
    "    - Skip-Gram model\n",
    "         - Predicts source context words ('the cat sits on the') from target words (e.g. 'mat').\n",
    "         - Treats each context-target pair as a new observation.\n",
    "         - Useful for larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- These algorithms are trained using:\n",
    "    - Negative sampling.\n",
    "    - Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![w2v](imgs/nce-nplm.png \"w2v\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgimenez/Dev/tools/miniconda3/envs/coolkids/lib/python3.4/site-packages/gensim/models/word2vec.py:456: UserWarning: C extension compilation failed, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\"C extension compilation failed, training will be slow. Install a C compiler and reinstall gensim for fast training.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "model = word2vec.Word2Vec(sentences_token, \n",
    "                          workers = 4,\n",
    "                          size = 300,        # Embeddings size  \n",
    "                          min_count = 1,     # How many times a word should appear to be taken into account\n",
    "                          window = 5, \n",
    "                          sample = 1e-3 ,    # Downsample setting for frequent words\n",
    "                         )\n",
    "\n",
    "# This model won't be updated\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('return', 0.9964651465415955),\n",
       " ('economic', 0.9960927963256836),\n",
       " ('recently', 0.9959506988525391),\n",
       " ('order', 0.9958515763282776),\n",
       " ('side', 0.9956673383712769),\n",
       " ('Stein', 0.9956244230270386),\n",
       " ('South', 0.9955523610115051),\n",
       " ('Johnny', 0.9954700469970703),\n",
       " ('gown', 0.9954312443733215),\n",
       " ('newly', 0.995410144329071)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "import scipy.stats as stats\n",
    "\n",
    "def get_most_common_vocab(most_common, vocabulary):\n",
    "    \"\"\" Get the most common words in a vocabulary\n",
    "\n",
    "    Args:\n",
    "        most_common (int): Number of most common word that want to be retrieved.\n",
    "        vocabulary (Counter): Vocabulary with words and frequencies of each word.\n",
    "        \n",
    "    Returns:\n",
    "        set: set of most common words in this vocabulary.\n",
    "    \"\"\"\n",
    "    most_common_words = vocabulary.most_common(int(most_common))\n",
    "    return set(word for word, _ in most_common_words)\n",
    "\n",
    "def get_vocabulary(corpus, tokenizer):\n",
    "    \"\"\" Get the vocabulary of a dataset. \n",
    "\n",
    "    Get a vocabulary of a set of tweets after removing stopwords, non letters, \n",
    "    and replacing each number by the token <number>\n",
    "\n",
    "    Args:\n",
    "        corpus (list of tweets): A list of tweets.\n",
    "        tokenizer (function): tokenizer function. To get the tokens of each tweet.\n",
    "\n",
    "    Returns:\n",
    "        Counter: Vocabulary with the frequency of each word in it.\n",
    "    \"\"\"\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Remove puntuation marks\n",
    "    no_punks = [re.sub(r'\\W', ' ', tweet) for tweet in corpus]\n",
    "    \n",
    "    # Tokenize and remove stop words\n",
    "    clean_tokens = []\n",
    "    for tweet in no_punks:\n",
    "        # Replace different numbers with a token\n",
    "        tweet = re.sub(r\"\\.\\d+\\s*\", \".<number> \", tweet)\n",
    "        tweet = re.sub(r\"\\d+\\s*\", \" <number> \", tweet)\n",
    "    \n",
    "        tokens = tokenizer(tweet)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        clean_tokens.extend(tokens)\n",
    "\n",
    "    # Build the vocabulary\n",
    "    return Counter(clean_tokens)\n",
    "\n",
    "\n",
    "def get_words_to_plot(most_common, vocabulary, dictionary):\n",
    "    words_to_plot = {}\n",
    "    unseen_words = []\n",
    "    for word in get_most_common_vocab(most_common, vocabulary):\n",
    "        if word in dictionary:\n",
    "            words_to_plot[word] = dictionary[word]\n",
    "        else:\n",
    "            unseen_words.append(word)\n",
    "    return words_to_plot, unseen_words\n",
    "\n",
    "def plot_tsne(dictionary, most_common):\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    tknzr = TweetTokenizer()\n",
    "    vocabulary = get_vocabulary(sentences, tknzr.tokenize)\n",
    "    words_to_plot, unseen_words = get_words_to_plot(most_common, vocabulary, dictionary)\n",
    "    \n",
    "    low_dim_embs = tsne.fit_transform(list(words_to_plot))\n",
    "    \n",
    "    range_words_bros=list(range(1,len(words_bros)+1))\n",
    "    source_bros = ColumnDataSource(data=dict(range_words=range_words_bros,\n",
    "                                             words=words_to_plot,\n",
    "                                             x=low_dim_embs[:,0], \n",
    "                                             y=low_dim_embs[:,1]))\n",
    "\n",
    "    hover = HoverTool()\n",
    "    hover.point_policy = \"follow_mouse\"\n",
    "    hover = HoverTool(\n",
    "            tooltips=[\n",
    "                (\"words_bros\", \"@words_bros\"),\n",
    "                (\"words_sis\", \"@words_sis\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    TOOLS=\"pan,wheel_zoom,box_zoom,reset,save\"\n",
    "\n",
    "    p = figure(title = \"Word visualization\", tools=[TOOLS, hover])\n",
    "    p.circle('x', 'y', source=source_bros, fill_alpha=0.2, size=10, color='navy')\n",
    "    p.circle('x', 'y', source=source_sis, fill_alpha=0.2, size=10, color='red')\n",
    "\n",
    "    show(p)\n",
    "    return set(unseen_words_bros + unseen_words_sis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
